# NRGYorkU Brain Computer Music Interface

### Goal:
To develop a Brain-Computer Music Interface (BCMI) to generate music based on a person's emotions, as determined by real-time EEG classification using an Emotiv kit.

### Requirements:
- Python 3.x. Anaconda installation highly recommended. Ability to run Jupyter Notebooks.
- MATLAB R2021a
- Required Python libraries will be added as needed 
- MNEPython for pre-processing EEG Data
- Emotiv EPOC Brain Activity Map

### Citations
Currently, the code in this repo consists entirely of [Stefan Erlich's music generation algorithm](https://github.com/stefan-ehrlich/code-algorithmicMusicGenerationSystem) and [Muhammad Nadzeri Munawar's Emotiv classifier](https://github.com/nadzeri/Realtime-EEG-Based-Emotion-Recognition). 

We intend on using the [DEAP dataset](http://www.eecs.qmul.ac.uk/mmv/datasets/deap/) for classifier training in the future.

Ehrlich, S. K., Agres, K. R., Guan, C., & Cheng, G. (2019). A closed-loop, music-based brain-computer interface for emotion mediation. PloS one, 14(3), e0213516. URL/DOI: https://doi.org/10.1371/journal.pone.0213516

